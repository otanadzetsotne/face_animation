# [SadTalker](https://github.com/OpenTalker/SadTalker)

1. **Используемые модели**. В проекте используется несколько моделей для создания видео «говорящей головы» из одного изображения и звука. Некоторые из этих моделей включают в себя:
    - **MappingNet**: предварительно обученная сеть MappingNet, используемая в SadTalker для сопоставления характеристик звука с коэффициентами движения лица.
    - **ExpNet**: предварительно обученная ExpNet в SadTalker для прогнозирования выражений по аудио.
    - **PoseVAE**: PoseVAE предварительно обучена в SadTalker для прогнозирования позы по звуку.
    - **модель face-vid2vid**: предварительно обученная модель face-vid2vid для синтеза видео.
    - **Экстрактор 3DMM**: используется для извлечения 3D-морфируемых моделей.
    - **Wav2Lip**: модель синхронизации губ, обеспечивающая точное движение губ в соответствии со звуком.
    - **Модели обнаружения и улучшения лиц**: используются для улучшения качества лица в создаваемых видео.

2. **Спецификации модели**: Репозиторий содержит подробные спецификации модели в папке `checkpoints` и сценарии модели в `src/audio2exp_models` и `src/audio2pose_models`. Также упоминаются конкретные детали и конфигурации модели, такие как использование Mapping_00109-model.pth.tar и Mapping_00229-model.pth.tar для MappingNet, а также различные конфигурации для старых и новых версий моделей.

3. **Необходимые зависимости**. Зависимости проекта перечислены в файлах `requirements.txt`, `req.txt` и `requirements3d.txt`. Ключевые зависимости включают `numpy`, `torch`, `torchvision`, `torchaudio`, `librosa`, `imageio`, `scipy`, `kornia`, `pyyaml`, `face_alignment`, `basicsr`, `facexlib`, `gfpgan`.

4. **Входные данные**: модель ожидает в качестве входных данных одно портретное изображение и аудиофайл. 

5. **Создание видео говорящей головы**: Проект позволяет создавать видео говорящей головы, используя только изображение и звук.

6. **Отличительные особенности**:
    - **Интеграция в Discord**: SadTalker можно бесплатно использовать в Discord, отправляя файлы.
    - **Расширение Stable-diffusion-webui**: проект интегрирован как расширение для стабильного-diffusion-webui.
    - **Режим полного изображения**: позволяет создавать анимацию всего тела в дополнение к анимации лица.
    - **Поддержка различных режимов**: включая режимы «Неподвижный», «Справочный» и «Изменение размера» для разных типов создания видео.

# [SyncTalk](https://github.com/ZiqiaoPeng/SyncTalk)

В репозитории **SyncTalk** представлен метод синтеза синхронизированных видео говорящих голов с упором на сохранение личности объекта, синхронизацию губ и естественное выражение лица.

1. **Используемые модели**: он использует трехплоскостные хэш-представления и аудиовизуальный кодировщик для создания видео говорящей головы высокого разрешения с синхронизированными движениями губ и выражениями лица.

2. **Спецификации модели**: Репозиторий объединяет различные пользовательские модули для обработки звука, кодирования лиц и рендеринга видео, но не детализирует такие спецификации, как количество слоев или параметры модели, непосредственно в README.

3. **Необходимые зависимости**: Ключевые зависимости включают PyTorch, OpenCV, TensorBoardX, numpy, pandas, tqdm, matplotlib, PyMCubes, scipy, scikit-learn, face_alignment и другие, указанные в файле `requirements.txt`.

4. **Входные данные**: для обучения модели требуется набор данных, состоящий из изображений и соответствующих аудиофайлов. В частности, в нем упоминается размещение файлов данных, таких как May.zip и trial_may.zip, в назначенных папках для обработки.

5. **Создание видео с говорящей головой**: Да, проект предназначен для создания видео с говорящей головой, используя только изображение и аудиовход. Он синтезирует синхронизированные видео говорящих голов, создавая синхронизацию движений губ, выражений лица и стабильных поз головы, восстанавливая детали волос для создания видео высокого разрешения.

6. **Отличительные особенности**:
    - Использует трехплоскостные хэш-представления для расширенного синтеза видео говорящих голов.
    - Основное внимание уделяется аспектам синхронизации, таким как движения губ и выражения лица, чтобы сохранить идентичность объекта в созданном видео.
    - Обеспечивает вывод видео высокого разрешения с детальным восстановлением волос.
    - Обеспечивает демонстрации ноутбуков Colab для быстрого доступа и экспериментирования с технологией.
    - База кода объединяет различные пользовательские модули Python для различных аспектов синтеза видео, таких как частотное кодирование («freqencoder»), сеточное кодирование («gridencoder»), кодирование сферических гармоник («shencoder») и марширование лучей («raymarching»). .

# [wunjo](https://github.com/wladradchenko/wunjo.wladradchenko.ru)

1. **Используемые модели**: проект включает в себя несколько моделей и инструментов для широкого спектра функций, включая Tacotron 2, Waveglow, клонирование голоса в реальном времени, VoiceFixer для синтеза речи и клонирования голоса; Open-Unmix для обработки звука; Wav2Lip, Face Utils для анимации и улучшения лица; Real-ESRGAN для улучшения изображений и видео; Segment Anything, ControlNet для обработки и сегментации видео; и адаптация Stable Diffusion.

2. **Спецификации модели**: Репозиторий не указывает такие детали, как количество слоев или параметры каждой модели, непосредственно в README. Детали реализации будут частью упомянутых конкретных модулей и адаптаций.

3. **Необходимые зависимости**: зависимости проекта перечислены в отдельных файлах требований для сред ЦП, графического процессора и macOS. Общие зависимости включают Flask, PyYAML, numpy, Torch, torchvision, libROSA, face_alignment, gfpgan, onnxruntime, praat-parselmouth и некоторые другие, что указывает на значительную зависимость от обработки звука, операций нейронных сетей и библиотек обработки изображений.

4. **Входные данные**. Приложение предназначено для работы с различными типами входных данных, включая текст для синтеза речи, аудиофайлы для клонирования голоса, а также изображения или видео для создания глубокой анимации и улучшений.

5. **Создание видео «Говорящая голова» из изображения и аудио**: Да, проект поддерживает анимацию лиц с использованием всего одной фотографии в сочетании со звуком, обеспечивая точную синхронизацию губ со звуком с помощью технологии deepfake. Он также позволяет менять лица в видео, GIF-файлах и фотографиях, используя всего одну фотографию.

6. **Отличительные особенности проекта**:
    - **Многоязычная поддержка**: поддерживает клонирование голоса на нескольких языках и синтез речи на английском, русском и китайском языках.
    - **Преобразование видео в видео по текстовой подсказке**: позволяет изменять форму видео с помощью текстовых подсказок, используя различные модели стабильного распространения.
    - **Deepfake Animation**: включает функции анимации лиц, точной синхронизации губ, замены лиц и изменения эмоций на основе текстовых описаний.
    - **Инструмент AI Retouch**: улучшает видео, удаляя ненужные объекты или улучшая качество дипфейков.
    - **Маска автоматической сегментации**: позволяет выбирать любой объект в любой период времени для детального редактирования и улучшения видео.

Этот репозиторий отличается всесторонним охватом потребностей голосового и визуального искусственного интеллекта и предлагает комплексное решение.

# [IP_LAP](https://github.com/Weizhi-Zhong/IP_LAP)

1. **Используемые модели:**
    - Генератор ориентиров: специальная реализация для создания ориентиров на основе входных изображений.
    - Video Renderer: визуализирует видео на основе ориентиров и опорных кадров.
    - Обработка звука: имеется модуль, посвященный обработке звука, что указывает на его важность в рабочем процессе проекта.

2. **Характеристики модели:**
    - Репозиторий содержит модели для создания ориентиров (`landmark_generator.py`) и рендеринга видео (`video_renderer.py`), а также обработки звука (`audio.py`).

3. **Необходимые зависимости:**
    - Зависимости проекта перечислены в `requirements.txt`, включая PyTorch, torchvision, scikit-image, scipy, numpy, librosa для обработки звука и многое другое.

4. **Ожидаемые данные:**
    - Хотя в README явно не упоминается формат входных данных, наличие сценариев предварительной обработки аудио (`preprocess_audio.py`) и видео (`preprocess_video.py`), а также модели генерации ориентиров, предполагает, что модель ожидает видео, кадры/изображения и соответствующий звук в качестве входных данных.

5. **Создание видео «Говорящая голова»:**
    - Проект, по-видимому, облегчает создание видео «говорящей головы» из изображения и звука. Он включает в себя этапы предварительной обработки видео и аудио, модели для создания ориентиров и рендеринга видео, указывая конвейер от статических изображений и аудио к видеовыходу.

6. **Необходимость дополнительного обучения:**
    - Скрипты обучения (`train_landmarks_generator.py`, `train_video_renderer.py`) предполагают, что для пользовательских приложений или улучшений возможно дополнительное обучение. Однако без четких инструкций неясно, предоставляются ли предварительно обученные модели или обучение является обязательным условием для первого использования.

7. **Отличительные особенности:**
    - Пользовательские модели для создания ориентиров и рендеринга видео.
    - Включение предварительной обработки звука.

---

# [Awesome-Talking-Head-Synthesis](https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis)

***repository - collection of face video generation***

# [Talk3D](https://github.com/KU-CVLAB/Talk3D)

Репозиторий Talk3D содержит реализацию платформы Talk3D, которая нацелена на синтез высококачественных говорящих портретов с помощью генеративного 3D-подхода.

[Статья](https://arxiv.org/abs/2403.20153v1)

1. **Используемые модели**: в проекте используется новая архитектура трехмерных моделей для синтеза портретов говорящей головы. Конкретные модели или архитектуры моделей, упомянутые в репозитории, включают EG3D (Explicit Generative 3D) и, возможно, компоненты, связанные со StyleGAN2 или StyleGAN3, о чем свидетельствует наличие сетевых файлов и подтверждений в README. Кроме того, использование преобразователей и различных слоев нейронных сетей, таких как GroupNorm, LeakyReLU, и пользовательских блоков, таких как SynthesisBlock, предполагает сложную и адаптированную архитектуру для решения этой задачи.

2. **Характеристики модели**: Характеристики модели не указаны напрямую в анализируемых файлах. Однако наличие блоков синтеза, преобразователей и ссылок на генерирующие 3D-модели подразумевают сложную архитектуру, предназначенную для создания портретов с высокой точностью и поддержкой 3D. Использование распределенных слоев, механизмов внимания и блоков обработки указывает на расширенные возможности управления и создания 3D-контента.

3. **Необходимые зависимости**. Для выполнения проекта требуется множество зависимостей, включая PyTorch, torchvision, torchaudio и множество других библиотек, которые поддерживают обработку изображений, разработку моделей глубокого обучения и обработку звука. Эти зависимости указаны в файле `requirements.txt`.

4. **Данные, ожидаемые в качестве входных данных**. Проект, скорее всего, ожидает, что 3D-данные или многоэкранные изображения наряду с аудиовходами будут синтезировать видео «говорящей головы». Этот вывод основан на структуре кода и использовании компонентов, поддерживающих 3D, хотя явные форматы ввода данных не подробно описаны в анализируемом контенте.

5. **Создание видео говорящей головы из изображения и аудио**: Похоже, проект способен синтезировать видео говорящей головы из изображений и аудиовходов, используя персонализированную 3D-генеративную априорную обработку. Эта возможность подразумевается целью проекта и описанными методологиями, ориентированными на высококачественный синтез говорящего портрета.

6. **Необходимость дополнительного обучения перед использованием**. Сценарии обучения и ссылки на наборы обучающих данных предполагают, что пользователям может потребоваться дополнительное обучение или тонкая настройка для достижения оптимальных результатов с конкретными наборами данных или для дальнейшей персонализации моделей.

7. **Отличительные особенности проекта**. Ключевые особенности включают в себя высокоточный синтез, генеративное моделирование с поддержкой 3D и возможности персонализации. Проект отличается акцентом на синтезе говорящих портретов с использованием генеративного 3D-подхода с целью создания более реалистичных и выразительных результатов, чем типичные 2D-подходы.

# [AniPortrait](https://github.com/Zejun-Yang/AniPortrait)

Проект AniPortrait предназначен для создания высококачественных фотореалистичных портретных анимаций на основе аудиовхода и эталонного портретного изображения. Он также может воспроизводить лицо с помощью видео.

[Статья](https://arxiv.org/abs/2403.17694)

1. **Используемые модели**: AniPortrait использует комбинацию моделей, включая модель Audio2Mesh для преобразования звука в движения сетки лица, U-Net для генерации изображений.

2. **Спецификации моделей**. Точные характеристики этих моделей, такие как системные требования и детали их архитектуры, разбросаны по разным проектам.

3. **Необходимые зависимости**. Зависимости включают «librosa», «numpy», «transformers», «diffusion» и многие другие, облегчающие операции с моделями глубокого обучения, обработку звука и манипулирование изображениями. Они перечислены в файле `requirements.txt`.

4. **Данные, ожидаемые в качестве входных данных**: модель ожидает аудиофайл (рекомендуется формат WAV) и эталонное портретное изображение в качестве входных данных для создания анимации. Для реконструкции лица вместе с эталонным изображением требуется исходное видео.

5. **Создание видео говорящей головы из изображения и аудио**: Да, проект поддерживает создание видео говорящей головы из аудио и одного изображения. Он синтезирует выражения лица и движения головы, соответствующие аудиовходу, отражая эту возможность в своих сценариях и конфигурациях.

6. **Необходимость дополнительного обучения перед использованием**: Хотя упоминаются предварительно обученные модели, README предполагает, что дальнейшая оптимизация и обучение могут повысить производительность, особенно для модели «аудио-поза», что указывает на то, что дополнительное обучение может быть полезным. или необходимо для конкретных приложений.

7. **Отличительные особенности проекта**: AniPortrait отличается высоким качеством и универсальностью решений, включая как аудио-анимацию, так и реконструкцию лиц. Он объединяет передовые модели глубокого обучения для обработки звука и изображения, предлагая комплексную основу для портретной анимации.

# [AdaSR-TalkingHead](https://github.com/Songluchuan/AdaSR-TalkingHead/)

[Статья](https://arxiv.org/abs/2403.15944v1)

1. **Используемые модели**. В проекте используется новый подход к созданию высококачественных видеороликов «говорящей головы» из одного изображения с упором на адаптивное сверхразрешение без необходимости использования дополнительных предварительно обученных модулей. Этот подход повышает четкость видео за счет модуля кодера-декодера, который восстанавливает высокочастотные детали.

2. **Характеристики модели**: метод подробно описан в документе для конференции ICASSP2024. Модель субдискретизирует однокадровое исходное изображение, а затем адаптивно восстанавливает высокочастотные детали с помощью модуля кодера-декодера.

3. **Необходимые зависимости**:
    - CUDA 11,3-11,6
    - PyTorch 1.10.1
    - matplotlib 3.4.3; 3.4.2
    - opencv-python 4.7.0
    - scikit-learn 1.0
    - tqdm 4.62.3
    — Дополнительные зависимости можно найти в файле `environment.yaml`.

4. **Ожидаемые входные данные**: модель ожидает, что портретное изображение из одного источника и видео будут синтезировать видео говорящей головы. Конкретные детали можно настроить в сценарии `run_demo.sh`, который ссылается на демонстрационные изображения и видео, включенные в каталог `DEMO/`.

5. **Создание видео говорящей головы из изображения и аудио**: Да, проект позволяет создавать видео говорящей головы только из изображения и звука. Этот процесс включает в себя использование изображения из одного источника и видео движения (которое может быть получено из аудио для целей синхронизации губ), как показано в их коде вывода и демонстрационном видео.

6. **Необходимость дополнительного обучения перед использованием**: предоставляются код вывода и предварительно обученная модель, поэтому дополнительное обучение не требуется для использования модели как есть. Однако для пользовательских наборов данных или дальнейших улучшений может потребоваться дополнительное обучение, и код обучения будет опубликован в ближайшее время.

7. **Отличительные особенности проекта**:
    - Он синтезирует видео высокого разрешения без необходимости использования дополнительных модулей сверхвысокого разрешения, сохраняя тем самым эффективность вычислений.
    - Проект основан на адаптивной реконструкции высокочастотных деталей, что существенно улучшает качество генерируемых видеороликов.
    - Подход прост, но эффективен, что подтверждается количественными и качественными оценками.

---

# [GeneFace](https://github.com/yerfor/GeneFace)

*(Необходима тренировка модели)*

1. **Используемые модели**: проект GeneFace использует комбинацию моделей для создания 3D-синтеза говорящего лица на основе аудиовхода. В нем упоминается использование NeRF (Neural Radiance Fields) для высокоточной генерации лиц, включая RAD-NeRF для вывода в реальном времени и повышения эффективности. Кроме того, он использует модели для преобразования звука в движение (audio2motion) и звука в позу (audio2pose), что указывает на сложную систему моделей, работающих вместе.

2. **Спецификации модели**. Конкретные конфигурации или архитектурные детали моделей разбросаны по разным файлам и не сведены в одном месте явно. Однако репозиторий содержит несколько скриптов Python для вывода, указывающих на сложные конвейеры модели (например, `audio2motion_infer.py`, `audio2pose_infer.py`, `lm3d_nerf_infer.py` и т. д.). Эти сценарии намекают на сложное взаимодействие между обработкой звука, прогнозированием 3D-ориентиров и рендерингом на основе NeRF.

3. **Необходимые зависимости**: в файле `requirements.txt` перечислены многочисленные пакеты Python, необходимые для проекта, включая `numpy`, `pandas`, `transformers`, `scipy`, `scikit-learn`, `opencv_python`. `, `face_alignment`, `librosa`, `praat-parselmouth`, `trimesh`, `kornia`, `ffmpeg-python` и другие. Такой широкий спектр зависимостей указывает на комплексный подход проекта к обработке звука, машинному обучению и 3D-моделированию.

4. **Входные данные**. Модели ожидают, что аудиовход станет основным источником данных для создания видео с говорящей головой. Звук обрабатывается для извлечения таких функций, как мел-спектрограммы и высота звука, которые затем используются для управления движением трехмерных ориентиров и оценки позы, что в конечном итоге приводит к созданию трехмерных говорящих лиц.

5. **Создание видео говорящей головы из изображения и аудио**: Проект позволяет создавать видео говорящей головы только из изображения и звука. Предоставленные предварительно обученные модели и сценарии упрощают процесс создания видео «говорящей головы», обрабатывая входной звук для анимации лица целевого человека на изображении.

6. **Отличительные особенности проекта**: GeneFace отличается тем, что фокусируется на высококачественном и обобщенном синтезе говорящих лиц на основе звука. Он использует передовые методы NeRF для реалистичного рендеринга, поддерживает вывод в реальном времени через RAD-NeRF и включает обработку звука с учетом высоты тона для улучшения синхронизации губ и выразительности. В проекте также подчеркивается простота использования и эффективность благодаря обновлениям, которые значительно ускоряют обработку и сокращают использование памяти.

Для более детального изучения рекомендуется просмотреть конкретные сценарии Python, связанные с выводом моделей и обучением, в репозитории.

# [sd-wav2lip-uhq](https://github.com/numz/sd-wav2lip-uhq)

Репозиторий **sd-wav2lip-uhq** расширяет инструмент Wav2Lip для создания видео с синхронизацией губ по аудиовходам, включая Stable Diffusion WebUI от Automatic1111 для улучшения качества видео.

1. **Используемые модели**: для синхронизации губ в основном используется модель Wav2Lip. Он вносит улучшения и интегрируется с инструментами Stable Diffusion для постобработки и улучшения качества видео. Кроме того, в нем упоминается использование GFPGAN и технологий замены лиц для конкретных улучшений.

2. **Спецификации модели**: проект объединяет различные модели и инструменты, но не определяет детали внутренней архитектуры в README. Подробные работы и спецификации будут распределены по сценариям и файлам кода в репозитории, особенно в `scripts/wav2lip/models/wav2lip.py` для реализации модели Wav2Lip.

3. **Необходимые зависимости**. Зависимости включают в себя такие библиотеки, как numpy, opencv-python, scipy, requests, Pillow, librosa и другие, перечисленные в requirements.txt. `файл. 

4. **Входные данные**: инструмент ожидает в качестве входных данных видео и аудиофайл. Видео используется для сопоставления и синхронизации движений лица с предоставленным звуком для создания реалистичного видео с синхронизацией губ.

5. **Создание видео говорящей головы из просто изображения и аудио**: Не совсем, проект предназначен для создания видео говорящей головы путем синхронизации движений губ в видео с входным звуком, улучшая качество изображения за счет постобработки Stable Diffusion. обработка.

6. **Отличительные особенности проекта**:
    - Интеграция со Stable Diffusion для улучшения качества видео.
    - Поддерживает управление несколькими проектами, замену нескольких лиц в одном кадре и работу с видеовходами высокого разрешения.
    - Представлены такие функции, как управление ключевыми кадрами для лучшего контроля над генерацией видео и интеграция Coqui TTS для функций преобразования текста в речь.
    - Предоставляет автономную версию для более расширенных функций и удобства.
